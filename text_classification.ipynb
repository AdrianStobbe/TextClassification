{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h2q27gKz1H20"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TUfAcER1oUS6"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gb7qyhNL1yWt"
   },
   "source": [
    "# Text classification with TensorFlow Lite model customization with TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fw5Y7snSuG51"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/tensorflow_examples/lite/model_customization/demo/text_classification.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/tensorflow_examples/lite/model_customization/demo/text_classification.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sr3q-gvm3cI8"
   },
   "source": [
    "The TensorFlow Lite model customization library simplifies the process of adapting and converting a TensorFlow neural-network model to particular input data when deploying this model for on-device ML applications.\n",
    "\n",
    "This notebook shows an end-to-end example that utilizes this model customization library to illustrate the adaption and conversion of a commonly-used text classification model to classify movie reviews on a mobile device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bcLF2PKkSbV3"
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "To run this example, we first need to install serveral required packages, including model customization package that in github [repo](https://github.com/tensorflow/examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qhl8lqVamEty"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling grpcio-1.26.0:\n",
      "  Successfully uninstalled grpcio-1.26.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping tensorflow as it is not installed.\n",
      "WARNING: Skipping google-colab as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-nightly in c:\\users\\adrian\\anaconda3\\lib\\site-packages (2.1.0.dev20191204)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from tf-nightly) (3.1.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from tf-nightly) (0.33.6)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from tf-nightly) (0.1.8)\n",
      "Requirement already satisfied: gast==0.2.2 in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from tf-nightly) (0.2.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from tf-nightly) (1.16.5)\n",
      "Requirement already satisfied: astor>=0.6.0 in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from tf-nightly) (0.8.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from tf-nightly) (0.8.1)\n",
      "Requirement already satisfied: tb-nightly<2.2.0a0,>=2.1.0a0 in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from tf-nightly) (2.1.0a20191206)\n",
      "Requirement already satisfied: tf-estimator-nightly in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from tf-nightly) (2.0.0.dev2019120709)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from tf-nightly) (1.0.8)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from tf-nightly) (1.11.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from tf-nightly) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from tf-nightly) (3.10.0)\n",
      "Collecting grpcio>=1.8.6 (from tf-nightly)\n",
      "  Using cached https://files.pythonhosted.org/packages/8b/14/ab1501cfff78b88d7368659b227c603d7599dd25226ff682c71334e78aed/grpcio-1.26.0-cp37-cp37m-win_amd64.whl\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from tf-nightly) (1.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from tf-nightly) (1.12.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly) (41.4.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly) (0.4.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly) (0.16.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly) (2.22.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly) (1.7.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly) (3.1.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.8->tf-nightly) (2.9.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly) (1.3.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly) (2.8)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly) (4.0)\n",
      "Requirement already satisfied: cachetools<3.2,>=2.0.0 in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly) (3.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly) (0.2.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\adrian\\anaconda3\\lib\\site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly) (0.4.8)\n",
      "Installing collected packages: grpcio\n",
      "Successfully installed grpcio-1.26.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y tensorflow google-colab grpcio\n",
    "!pip install tf-nightly\n",
    "#!pip install -q git+https://github.com/tensorflow/examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l6lRhVK9Q_0U"
   },
   "source": [
    "Import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XtxiUeZEiXpt"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "import sys\n",
    "sys.path.append('C:/Users/Adrian/Documents/examples')\n",
    "\n",
    "from tensorflow_examples.lite.model_customization.core.data_util.text_dataloader import TextClassifierDataLoader\n",
    "from tensorflow_examples.lite.model_customization.core.model_export_format import ModelExportFormat\n",
    "import tensorflow_examples.lite.model_customization.core.task.text_classifier as text_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "06sWWfvE6I8e"
   },
   "source": [
    "## Simple End-to-End Example\n",
    "\n",
    "Let's get some texts to play with this simple end-to-end example. You could replace it with your own text folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R2BSkxWg6Rhx"
   },
   "outputs": [],
   "source": [
    "#data_path = tf.keras.utils.get_file(\n",
    "#      fname='aclImdb',\n",
    "#      origin='http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz',\n",
    "#      untar=True)\n",
    "data_path = \"C:/Users/Adrian/Documents/BigData THU/aclImdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WlKU3SMX6TnB"
   },
   "source": [
    "The example just consists of 4 lines of code as shown below, each of which representing one step of the overall process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s5U-A3tw6Y27"
   },
   "source": [
    "1.   Load train and test data specific to an on-device ML app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HD5BvzWe6YKa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:load text from C:\\Users\\Adrian\\Documents\\BigData THU\\aclImdb\\train with size: 25000, num_label: 2, labels: neg, pos\n",
      "INFO:tensorflow:load text from C:\\Users\\Adrian\\Documents\\BigData THU\\aclImdb\\test with size: 25000, num_label: 2, labels: neg, pos\n"
     ]
    }
   ],
   "source": [
    "train_data = TextClassifierDataLoader.from_folder(os.path.join(data_path, 'train'), class_labels=['pos', 'neg'])\n",
    "test_data = TextClassifierDataLoader.from_folder(os.path.join(data_path, 'test'), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2uZkLR6N6gDR"
   },
   "source": [
    "2. Customize the TensorFlow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kwlYdTcg63xy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Retraining the models...\n",
      "Train for 625 steps, validate for 78 steps\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 39s 63ms/step - loss: 0.5627 - accuracy: 0.7243 - val_loss: 0.3546 - val_accuracy: 0.8666\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 37s 59ms/step - loss: 0.3157 - accuracy: 0.8763 - val_loss: 0.2799 - val_accuracy: 0.8934A: 17s -  - ETA: 15s - loss - ETA: 13s - loss: 0.3380 - - ETA: 11s - loss: 0.3355 - - ETA: 10s - loss: 0.3329 - accuracy: - ET - ETA: 1s - ETA: 0s - loss: 0.3161 - accuracy: 0.\n"
     ]
    }
   ],
   "source": [
    "model = text_classifier.create(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-BzCHLWJ6h7q"
   },
   "source": [
    "3. Evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8xmnl6Yy7ARn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 97s 124ms/step - loss: 0.3203 - accuracy: 0.8674\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CgCDMe0e6jlT"
   },
   "source": [
    "4.  Export to TensorFlow Lite  model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hm_UULdW7A9T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Export to tflite model movie_review_classifier.tflite, saved labels in text_label.txt.\n",
      "INFO:tensorflow:  Saved vocabulary in vocab.txt.\n"
     ]
    }
   ],
   "source": [
    "model.export('movie_review_classifier.tflite', 'text_label.txt', 'vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rVxaf3x_7OfB"
   },
   "source": [
    "After this simple 4 steps, we could further use TensorFlow Lite model file and label file in on-device applications like in [text classification](https://github.com/tensorflow/examples/tree/master/lite/examples/text_classification) reference app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l65ctmtW7_FF"
   },
   "source": [
    "## Detailed Process\n",
    "\n",
    "In above, we tried the simple end-to-end example. The following walks through the example step by step to show more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ygEncJxtl-nQ"
   },
   "source": [
    "### Step 1: Load Input Data Specific to an On-device ML App\n",
    "\n",
    "The IMDB dataset contains 25000 movie reviews for training and 25000 movie reviews for testing from the [Internet Movie Database](https://www.imdb.com/). The dataset have two classes: positive and negative movie reviews.\n",
    "\n",
    "Download the archive version of the dataset and untar it.\n",
    "\n",
    "The IMDB dataset has the following directory structure:\n",
    "\n",
    "<pre>\n",
    "<b>aclImdb</b>\n",
    "|__ <b>train</b>\n",
    "    |______ <b>pos</b>: [1962_10.txt, 2499_10.txt, ...]\n",
    "    |______ <b>neg</b>: [104_3.txt, 109_2.txt, ...]\n",
    "    |______ unsup: [12099_0.txt, 1424_0.txt, ...]\n",
    "|__ <b>test</b>\n",
    "    |______ <b>pos</b>: [1384_9.txt, 191_9.txt, ...]\n",
    "    |______ <b>neg</b>: [1629_1.txt, 21_1.txt]\n",
    "\n",
    "</pre>\n",
    "\n",
    "Note that the text data under `train/unsup` folder are unlabeled documents for unsupervised learning and such data should be ignored in this tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7tOfUr2KlgpU"
   },
   "outputs": [],
   "source": [
    "#data_path = tf.keras.utils.get_file(\n",
    "#      fname='aclImdb',\n",
    "#      origin='http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz',\n",
    "#      untar=True)\n",
    "data_path = \"C:/Users/Adrian/Documents/BigData THU/aclImdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E051HBUM5owi"
   },
   "source": [
    "Use `TextClassifierDataLoader` to load data.\n",
    "\n",
    "As for `from_folder()` method, it could load data from the folder. It assumes that the text data of the same class are in the same subdirectory and the subfolder name is the class name. Each text file contains one movie review sample.\n",
    "\n",
    "Parameter `class_labels` is used to specify which subfolder should be considered. As for `train` folder, this parameter is used to skip `unsup` subfolder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I_fOlZsklmlL"
   },
   "outputs": [],
   "source": [
    "train_data = TextClassifierDataLoader.from_folder(os.path.join(data_path, 'train'), class_labels=['pos', 'neg'])\n",
    "test_data = TextClassifierDataLoader.from_folder(os.path.join(data_path, 'test'), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OB1jBv9tQSzU"
   },
   "source": [
    "Take a glance at 25 training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r7wMxLEcnpM1"
   },
   "outputs": [],
   "source": [
    "for text, label in train_data.dataset.take(25):\n",
    "  print (\"%s: %s\"%(train_data.index_to_label[label.numpy()], text.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AWuoensX4vDA"
   },
   "source": [
    "### Step 2: Customize the TensorFlow Model\n",
    "\n",
    "Create a custom text classifier model based on the loaded data. Currently, we only supports averging word embedding method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TvYSUuJY3QxR"
   },
   "outputs": [],
   "source": [
    "model = text_classifier.create(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0JKI-pNc8idH"
   },
   "source": [
    "Have a look at the detailed model structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gd7Hs8TF8n3H"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LP5FPk_tOxoZ"
   },
   "source": [
    "### Step 3: Evaluate the Customized Model\n",
    "\n",
    "Evaluate the result of the model, get the loss and accuracy of the model.\n",
    "\n",
    "Evaluate the loss and accuracy in `test_data`. If no data is given the results are evaluated on the data that's splitted in the `create` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A8c2ZQ0J3Riy"
   },
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aeHoGAceO2xV"
   },
   "source": [
    "### Step 4: Export to TensorFlow Lite Model\n",
    "\n",
    "Convert the existing model to TensorFlow Lite model format that could be later used in on-device ML application. Meanwhile, save the text labels in label file and vocabulary in vocab file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Im6wA9lK3TQB"
   },
   "outputs": [],
   "source": [
    "model.export('movie_review_classifier.tflite', 'text_label.txt', 'vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w12kvDdHJIGH"
   },
   "source": [
    "The TensorFlow Lite model file and label file could be used in [text classification](https://github.com/tensorflow/examples/tree/master/lite/examples/text_classification) reference app.\n",
    "\n",
    "In detail, we could add `movie_review_classifier.tflite`, `text_label.txt` and `vocab.txt` in [assets](https://github.com/tensorflow/examples/tree/master/lite/examples/text_classification/android/app/src/main/assets) folder. Meanwhile, change the filenames in [code](https://github.com/tensorflow/examples/blob/master/lite/examples/text_classification/android/app/src/main/java/org/tensorflow/lite/examples/textclassification/TextClassificationClient.java#L43). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HZKYthlVrTos"
   },
   "source": [
    "Here, we also demonstrate how to use the above files to run and evaluate the TensorFlow Lite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ochbq95ZrVFX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Lite model accuracy = 0.8724\n"
     ]
    }
   ],
   "source": [
    "# Read TensorFlow Lite model from TensorFlow Lite file.\n",
    "with tf.io.gfile.GFile('movie_review_classifier.tflite', 'rb') as f:\n",
    "  model_content = f.read()\n",
    "\n",
    "# Read label names from label file.\n",
    "with tf.io.gfile.GFile('text_label.txt', 'r') as f:\n",
    "  label_names = f.read().split('\\n')\n",
    "\n",
    "# Initialze TensorFlow Lite inpterpreter.\n",
    "interpreter = tf.lite.Interpreter(model_content=model_content)\n",
    "interpreter.allocate_tensors()\n",
    "input_index = interpreter.get_input_details()[0]['index']\n",
    "output = interpreter.tensor(interpreter.get_output_details()[0][\"index\"])\n",
    "\n",
    "# Run predictions on each test data and calculate accuracy.\n",
    "accurate_count = 0\n",
    "for i, (text, label) in enumerate(model.test_data.dataset):\n",
    "    # Pre-processing should remain the same.\n",
    "    text, label = model.preprocess(text, label)\n",
    "    # Add batch dimension and convert to float32 to match with the model's input\n",
    "    # data format.\n",
    "    text = tf.expand_dims(text, 0).numpy()\n",
    "    text = tf.cast(text, tf.float32)\n",
    "\n",
    "    # Run inference.\n",
    "    interpreter.set_tensor(input_index, text)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Post-processing: remove batch dimension and find the label with highest\n",
    "    # probability.\n",
    "    predict_label = np.argmax(output()[0])\n",
    "    # Get label name with label index.\n",
    "    predict_label_name = label_names[predict_label]\n",
    "    \n",
    "    accurate_count += (predict_label == label.numpy())\n",
    "\n",
    "accuracy = accurate_count * 1.0 / model.test_data.size\n",
    "print('TensorFlow Lite model accuracy = %.4f' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KLKmboKFtgc2"
   },
   "source": [
    "Note that preprocessing for inference should be the same as training. Currently, preprocessing contains split the text to tokens by '\\W', encode the tokens to ids, the pad the text with `pad_id` to have the length of `sentence_length`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EoWiA_zX8rxE"
   },
   "source": [
    "# Advanced Usage\n",
    "\n",
    "The `create` function is the critical part of this library that contains the following steps:\n",
    "\n",
    "1.   Split the data into training, validation, testing data according to parameter `validation_ratio` and `test_ratio`. The default value of `validation_ratio` and `test_ratio` are `0.1` and `0.1`.\n",
    "2.   Tokenize the text and select the top `num_words` frequency of words to generate the vocubulary. The default value of `num_words` is `10000`.\n",
    "3.   Encode the text string tokens to int ids.\n",
    "4.   Create the text classifier model. Currently, this library supports one model: average the word embedding of the text with RELU activation, then leverage softmax dense layer for classification. As for [Embedding layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding), `input_dim` is the size of the vocabulary, `output_dim` is `create` function's paramater `wordvec_dim` which default value is `16`, `input_length` is `create` function's paramater `sentence_len` which default value is `256`.\n",
    "5.   Train the classifier model. The default epoch is `2` and the default batch size is `32`.\n",
    "\n",
    "In this section, we describe several advanced topics, including adjusting the model, changing the training hyperparameters etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mwtiksguDfhl"
   },
   "source": [
    "# Adjust the model\n",
    "\n",
    "We could adjust the model infrastructure like `wordvec_dim`, `sentence_len`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cAOd5_bzH9AQ"
   },
   "source": [
    "*   `wordvec_dim`: Dimension of word embedding.\n",
    "*   `sentence_len`: length of sentence.\n",
    "\n",
    "For example, we could train with larger `wordvec_dim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PzpV246_JGEu"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-27bb451470d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordvec_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_examples\\lite\\model_customization\\core\\task\\text_classifier.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(data, model_export_format, model_name, shuffle, batch_size, epochs, validation_ratio, test_ratio, num_words, sentence_len, wordvec_dim, dropout_rate, lowercase)\u001b[0m\n\u001b[0;32m     76\u001b[0m       \u001b[0mwordvec_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwordvec_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m       \u001b[0mdropout_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m       lowercase=lowercase)\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m   \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Retraining the models...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_examples\\lite\\model_customization\\core\\task\\text_classifier.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, model_export_format, model_name, shuffle, validation_ratio, test_ratio, num_words, sentence_len, wordvec_dim, dropout_rate, lowercase)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentence_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence_len\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gen_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordvec_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_examples\\lite\\model_customization\\core\\task\\text_classifier.py\u001b[0m in \u001b[0;36m_gen_vocab\u001b[1;34m(self, text_ds, num_words)\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[0mvocab_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mraw_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext_ds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m       \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# For Python 3 compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 630\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    631\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m     \u001b[1;34m\"\"\"Returns a nested structure of `Tensor`s containing the next element.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 674\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    675\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    657\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 659\u001b[1;33m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2466\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"IteratorGetNextSync\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2467\u001b[0m         \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"output_types\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2468\u001b[1;33m         \"output_shapes\", output_shapes)\n\u001b[0m\u001b[0;32m   2469\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2470\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = text_classifier.create(train_data, wordvec_dim=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LvQuy7RSDir3"
   },
   "source": [
    "## Change the training hyperparameters\n",
    "We could also change the training hyperparameters like `epochs` and `batch_size` that could affect the model accuracy. For instance,\n",
    "\n",
    "*   `epochs`: more epochs could achieve better accuracy until converage but training for too many epochs may lead to overfitting.\n",
    "*   `batch_size`: number of samples to use in one training step.\n",
    "\n",
    "For example, we could train with more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rnWFaYZBG6NW"
   },
   "outputs": [],
   "source": [
    "model = text_classifier.create(train_data, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nUaKQZBQHBQR"
   },
   "source": [
    "Evaluate the newly retrained model with 5 training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BMPi1xflHDSY"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "//learning/brain/python/client:colab_notebook_py3",
    "kind": "private"
   },
   "name": "Text Classification with TensorFlow Lite model customization.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1Yg8COhKoiCSyf8sL4nTUSFI5iTnG6usg",
     "timestamp": 1570624529844
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
